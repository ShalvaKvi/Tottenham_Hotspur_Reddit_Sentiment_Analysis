win = case_when(
score_tot > score_op ~ 1,
score_tot < score_op ~ 0,
score_tot == score_op ~ 0.5
)
)
View(results)
results <-
read_html('https://www.worldfootball.net/teams/tottenham-hotspur/2022/3/') |>
html_nodes(
".dunkel:nth-child(7) , .hell:nth-child(7) , .portfolio .dunkel:nth-child(2) a ,
.portfolio .hell:nth-child(2) , .dunkel:nth-child(6) , .hell:nth-child(6)"
) |>
html_text()
# Convert to a data.frame
results <-
matrix(results,
ncol = 3,
nrow = length(results) / 3,
byrow = TRUE) |>
as_tibble()
View(results)
# Wrangling the results a bit --- renaming columns, adjusting features, etc.
results <- results |>
rename("date" = "V1",
"opponent" = "V2",
"score" = "V3") |>
mutate(
opponent = str_replace_all(string = opponent, pattern = "\n",replacement = ""),
date = as.POSIXlt(date, format = "%d/%m/%Y"),
score = str_extract(string = score, pattern = "[:digit:]:[:digit:]"),
score_tot = as.numeric(str_extract(string = score, pattern = "^[:digit:]")),
score_op = as.numeric(str_extract(string = score, pattern = "[:digit:]$")),
win = case_when(
score_tot > score_op ~ 1,
score_tot < score_op ~ 0,
score_tot == score_op ~ 0.5
)
)
View(results)
pacman::p_load(
tidyverse, tidylog,
wordcloud2, RedditExtractoR, tidytext, SnowballC,
ggthemes, extrafont, ggraph, igraph)
# READ
results <- read.csv("results.csv")
comments <- read.csv("comments.csv")
threads <- read.csv('threads.csv')
tokens <- comments |>
select("text" = comment, gameid, "commentid" = comment_id) |>
unnest_tokens(word, text)
tokens <- tokens |>
anti_join(get_stopwords(source = "snowball"))
tokens <- tokens |>
mutate(stem = wordStem(word))
df <- comments |>
select("text" = comment, gameid, "commentid" = comment_id) |>
unnest_tokens(word, text) |>
add_count(word) |>
filter(n >= 50) |>
select(-n)
nested_words <- df %>%
nest(words = c(word))
### SENTIMENT PLOT
df <- comments |>
select("text" = comment, gameid, comment_id)
df <- df |>
unnest_tokens(word, text, token = "words", drop = FALSE) |>
select(!text) |>
anti_join(get_stopwords(language = "en", source = "snowball"))
df <- df |>
inner_join(get_sentiments("afinn")) |>
group_by(gameid, comment_id) |>
summarise(sentiment = mean(value)) |>
group_by(gameid) |>
summarise(sentiment = mean(sentiment))
results <- results |>
left_join(df, by = c("gameid" = "gameid")) |>
mutate(win_text = factor(case_when(win == 1 ~ "Win",
win == 0.5 ~ "Draw",
win == 0 ~ "Lose"),
levels = c("Win","Lose","Draw")))
plt <-
ggplot(data = results, aes(
x = gameid,
y = sentiment,
label = paste(opponent, score)
)) +
geom_hline(yintercept = 0) +
geom_segment(aes(color = win_text, yend = 0, xend = gameid),
lineend = "round",
size = 2) +
geom_point(aes(color = win_text, y = 0), size = 4) +
geom_segment(aes(yend = 0, xend = gameid)) +
geom_text(
aes(
color = win_text,
y = ifelse(sentiment > 0, 0.05, -0.05),
hjust = ifelse(sentiment > 0, 0, 1)
),
vjust = -0.5,
size = 2.5,
angle = 90
) +
scale_x_reverse() +
scale_color_manual(values = c("#0072B5", "#BC3C29", "#20854E")) +
labs(
title = "r/coys Post-Match Threads Average Sentiment",
y = "Average sentiment",
color = "Game outcome:",
caption = "Computed with AFINN sentiment lexicon. Finn Årup Nielsen. (2011)."
) +
theme_hc(base_size = 16) +
theme(
text = element_text(family = "Candara"),
axis.title.x = element_blank(),
axis.text.x = element_blank(),
axis.ticks.x = element_blank()
)
plt
# READ
results <- read.csv("results.csv")
comments <- read.csv("comments.csv")
threads <- read.csv('threads.csv')
tokens <- comments |>
select("text" = comment, gameid, "commentid" = comment_id) |>
unnest_tokens(word, text)
tokens <- tokens |>
anti_join(get_stopwords(source = "snowball"))
tokens <- tokens |>
mutate(stem = wordStem(word))
df <- comments |>
select("text" = comment, gameid, "commentid" = comment_id) |>
unnest_tokens(word, text) |>
add_count(word) |>
filter(n >= 50) |>
select(-n)
nested_words <- df %>%
nest(words = c(word))
### SENTIMENT PLOT
df <- comments |>
select("text" = comment, gameid, comment_id)
df <- df |>
unnest_tokens(word, text, token = "words", drop = FALSE) |>
select(!text) |>
anti_join(get_stopwords(language = "en", source = "snowball"))
df <- df |>
inner_join(get_sentiments("afinn")) |>
group_by(gameid, comment_id) |>
summarise(sentiment = mean(value)) |>
group_by(gameid) |>
summarise(sentiment = mean(sentiment))
install.package("textdata")
install.packages("textdata")
df <- df |>
inner_join(get_sentiments("afinn")) |>
group_by(gameid, comment_id) |>
summarise(sentiment = mean(value)) |>
group_by(gameid) |>
summarise(sentiment = mean(sentiment))
results <- results |>
left_join(df, by = c("gameid" = "gameid")) |>
mutate(win_text = factor(case_when(win == 1 ~ "Win",
win == 0.5 ~ "Draw",
win == 0 ~ "Lose"),
levels = c("Win","Lose","Draw")))
plt <-
ggplot(data = results, aes(
x = gameid,
y = sentiment,
label = paste(opponent, score)
)) +
geom_hline(yintercept = 0) +
geom_segment(aes(color = win_text, yend = 0, xend = gameid),
lineend = "round",
size = 2) +
geom_point(aes(color = win_text, y = 0), size = 4) +
geom_segment(aes(yend = 0, xend = gameid)) +
geom_text(
aes(
color = win_text,
y = ifelse(sentiment > 0, 0.05, -0.05),
hjust = ifelse(sentiment > 0, 0, 1)
),
vjust = -0.5,
size = 2.5,
angle = 90
) +
scale_x_reverse() +
scale_color_manual(values = c("#0072B5", "#BC3C29", "#20854E")) +
labs(
title = "r/coys Post-Match Threads Average Sentiment",
y = "Average sentiment",
color = "Game outcome:",
caption = "Computed with AFINN sentiment lexicon. Finn Årup Nielsen. (2011)."
) +
theme_hc(base_size = 16) +
theme(
text = element_text(family = "Candara"),
axis.title.x = element_blank(),
axis.text.x = element_blank(),
axis.ticks.x = element_blank()
)
plt
library(RedditExtractoR)
library(tidyverse)
library(RedditExtractoR)
# Fetch results from worldfootball.net
results <-
read_html('https://www.worldfootball.net/teams/tottenham-hotspur/2022/3/') |>
html_nodes(
".dunkel:nth-child(7) , .hell:nth-child(7) , .portfolio .dunkel:nth-child(2) a ,
.portfolio .hell:nth-child(2) , .dunkel:nth-child(6) , .hell:nth-child(6)"
) |>
html_text()
# Convert to a data.frame
results <-
matrix(results,
ncol = 3,
nrow = length(results) / 3,
byrow = TRUE) |>
as.data.frame()
# Wrangling the results a bit --- renaming columns, adjusting features, etc.
results <- results |>
rename("date" = "V1",
"opponent" = "V2",
"score" = "V3") |>
mutate(
opponent = str_replace_all(string = opponent, pattern = "\n",replacement = ""),
date = as.POSIXlt(date, format = "%d/%m/%Y"),
score = str_extract(string = score, pattern = "[:digit:]:[:digit:]"),
score_tot = as.numeric(str_extract(string = score, pattern = "^[:digit:]")),
score_op = as.numeric(str_extract(string = score, pattern = "[:digit:]$")),
win = case_when(
score_tot > score_op ~ 1,
score_tot < score_op ~ 0,
score_tot == score_op ~ 0.5
)
)
View(results)
daily_thread_search <- RedditExtractoR::find_thread_urls(keywords = "Daily Discussion Thread",
sort_by = "new",
subreddit = "coys",
period = "all")
View(daily_thread_search)
daily_thread_search_21 <- RedditExtractoR::find_thread_urls(keywords = "Daily Discussion Thread (2021)",
sort_by = "new",
subreddit = "coys",
period = "all")
View(daily_thread_search_21)
View(results)
user_content <- RedditExtractoR::get_user_content(users = "Professorchronic")
View(user_content)
t <- user_content$Professorchronic$threads
View(t)
View(daily_thread_search_21)
daily_thread_search_aug21 <- RedditExtractoR::find_thread_urls(keywords = "Daily Discussion Thread (August 2021)",
sort_by = "new",
subreddit = "coys",
period = "all")
View(daily_thread_search_aug21)
View(user_content)
View(t)
View(daily_thread_search_21)
daily_threads <- daily_thread_search |>
bind_rows(daily_thread_search_21)
write_csv(daily_threads, "daily_threads.csv")
library(reticulate)
library(tidyverse)
data = reticulate::py_load_object("G:\\Shared drives\\Technologies\\Development\\Data Insights\\db\\Formula E\\Data\\Processed\\R1_Race_1684_telemetry.pkl")
reticulate
reticulate::py_module_available()
reticulate::py_module_available(module = pandas)
reticulate::py_module_available(module = "pandas")
reticulate::import("pandas")
reticulate::py_install("pandas")
reticulate::import("pandas")
data = reticulate::py_load_object("G:\\Shared drives\\Technologies\\Development\\Data Insights\\db\\Formula E\\Data\\Processed\\R1_Race_1684_telemetry.pkl")
timing = reticulate::py_load_object("G:\\Shared drives\\Technologies\\Development\\Data Insights\\db\\Formula E\\Data\\Processed\\R1_Race_1684_loops.pkl.pkl")
data = reticulate::py_load_object("G:\\Shared drives\\Technologies\\Development\\Data Insights\\db\\Formula E\\Data\\Processed\\R1_Race_1684_telemetry.pkl")
timing = reticulate::py_load_object("G:\\Shared drives\\Technologies\\Development\\Data Insights\\db\\Formula E\\Data\\Processed\\R1_Race_1684_loops.pkl")
drivers <- unique(timing$driver_id)
data_clean <- data |>
filter(driver_id %in% drivers) |>
filter(TV_Batt_Cap > 0) |>
filter(TV_Transponder != 0)
data$Lap <- 0
data_clean$Lap <- 0
if(data_clean$driver_id[i] == data_clean$driver_id[i-1]){
if(data_clean$TV_Distance[i] >= data_clean$TV_Distance[i-1]){
data_clean$Lap[i] <- data_clean$Lap[i-1]
} else if(data_clean$TV_Distance[i] < data_clean$TV_Distance[i-1]){
data_clean$Lap[i] <- data_clean$Lap[i-1] + 1
}
}
for(i in 1:nrow(data_clean)){
if(data_clean$driver_id[i] == data_clean$driver_id[i-1]){
if(data_clean$TV_Distance[i] >= data_clean$TV_Distance[i-1]){
data_clean$Lap[i] <- data_clean$Lap[i-1]
} else if(data_clean$TV_Distance[i] < data_clean$TV_Distance[i-1]){
data_clean$Lap[i] <- data_clean$Lap[i-1] + 1
}
}
}
for(i in 2:nrow(data_clean)){
if(data_clean$driver_id[i] == data_clean$driver_id[i-1]){
if(data_clean$TV_Distance[i] >= data_clean$TV_Distance[i-1]){
data_clean$Lap[i] <- data_clean$Lap[i-1]
} else if(data_clean$TV_Distance[i] < data_clean$TV_Distance[i-1]){
data_clean$Lap[i] <- data_clean$Lap[i-1] + 1
}
}
}
View(data_clean)
data_clean <- data |>
filter(driver_id %in% drivers) |>
filter(TV_Batt_Cap > 0) |>
filter(TV_Transponder != 0) |>
arrange(driver_id, TV_absTime)
View(data_clean)
data_clean$Lap <- 0
for(i in 2:nrow(data_clean)){
if(data_clean$driver_id[i] == data_clean$driver_id[i-1]){
if(data_clean$TV_Distance[i] >= data_clean$TV_Distance[i-1]){
data_clean$Lap[i] <- data_clean$Lap[i-1]
} else if(data_clean$TV_Distance[i] < data_clean$TV_Distance[i-1]){
data_clean$Lap[i] <- data_clean$Lap[i-1] + 1
}
}
}
View(data_clean)
library(tidyverse)
library(sentimentr)
daily_threads_info <- read_csv('./data/daily_threads_info.csv')
results <- read_csv("./data/results.csv")
# Read threads, then bind them info a dataframe using 'purrr::map_dfr' and 'dplyr::bind_rows'
threads <- fs::dir_ls(path = './data/threads', glob = '*rds') |>
purrr::map(read_rds) |>
purrr::flatten_lgl()
# Read threads, then bind them info a dataframe using 'purrr::map_dfr' and 'dplyr::bind_rows'
threads <- fs::dir_ls(path = './data/threads', glob = '*rds') |>
purrr::map(read_rds) |>
purrr::flatten()
View(threads)
# Read threads, then bind them info a dataframe using 'purrr::map_dfr' and 'dplyr::bind_rows'
threads <- fs::dir_ls(path = './data/threads', glob = '*rds') |>
purrr::map(read_rds) |>
purrr::list_merge()
View(threads)
# Read threads, then bind them info a dataframe using 'purrr::map_dfr' and 'dplyr::bind_rows'
threads <- fs::dir_ls(path = './data/threads', glob = '*rds') |>
purrr::map_dfr(read_rds) |>
purrr::list_merge()
View(threads)
threads$threads
?map_dfr
# Read threads, then bind them info a dataframe using 'purrr::map_dfr' and 'dplyr::bind_rows'
threads <- fs::dir_ls(path = './data/threads', glob = '*rds') |>
purrr::map_dfr(read_rds)
View(threads)
# Clean deleted or removed comments
threads$comments <- threads$comments |>
filter(str_detect(comment, pattern = '[removed]') != '[removed]')
library(tidylog)
library(sentimentr)
daily_threads_info <- read_csv('./data/daily_threads_info.csv')
results <- read_csv("./data/results.csv")
# Read threads, then bind them into a nested dataframe using 'purrr::map_dfr'
threads <- fs::dir_ls(path = './data/threads', glob = '*rds') |>
purrr::map_dfr(read_rds)
# Clean deleted or removed comments
threads$comments <- threads$comments |>
filter(str_detect(comment, pattern = '[removed]') != '[removed]')
threads$comments$comment
# Clean deleted or removed comments
threads$comments <- threads$comments |>
filter(str_detect_all(comment, pattern = '[removed]') != '[removed]')
# Clean deleted or removed comments
threads$comments <- threads$comments |>
filter(str_extract(comment, pattern = '[removed]') != '[removed]')
comments <- threads$comments
threads <- threads$threads
View(threads)
# Read threads, then bind them into a nested dataframe using 'purrr::map_dfr'
threads <- fs::dir_ls(path = './data/threads', glob = '*rds') |>
purrr::map_dfr(read_rds)
comments <- threads$comments
threads <- threads$threads
# Clean deleted or removed comments
comments <- comments |>
filter(str_extract(comment, pattern = '[removed]') != '[removed]')
# Clean deleted or removed comments
comments <- comments |>
filter(str_extract(comment, pattern = '[removed]') != '[removed]') |>
filter(str_extract(comment, pattern = '[deleted]') != '[deleted]')
daily_threads_info <- read_csv('./data/daily_threads_info.csv')
results <- read_csv("./data/results.csv")
# Read threads, then bind them into a nested dataframe using 'purrr::map_dfr'
threads <- fs::dir_ls(path = './data/threads', glob = '*rds') |>
purrr::map_dfr(read_rds)
comments <- threads$comments
threads <- threads$threads
# Clean deleted or removed comments
comments <- comments |>
filter(str_match(comment, pattern = '[removed]') != '[removed]') |>
filter(str_match(comment, pattern = '[deleted]') != '[deleted]')
View(comments)
View(comments)
comments$comment[11]
comments$comment[10]
library(tidyverse)
library(tidylog)
library(sentimentr)
daily_threads_info <- read_csv('./data/daily_threads_info.csv')
results <- read_csv("./data/results.csv")
# Read threads, then bind them into a nested dataframe using 'purrr::map_dfr'
threads <- fs::dir_ls(path = './data/threads', glob = '*rds') |>
purrr::map_dfr(read_rds)
comments <- threads$comments
threads <- threads$threads
# Clean deleted or removed comments
comments <- comments |>
filter(str_match(comment, pattern = '[removed]') != '[removed]') |>
filter(str_match(comment, pattern = '[deleted]') != '[deleted]')
View(comments)
str_match(comments$comment[10], '[deleted]')
str_match(comments$comment[10], 'deleted')
str_match(comments$comment[10], '//[deleted//]')
str_match(comments$comment[10], '[')
str_match(comments$comment[10], '\\[')
str_match(comments$comment[10], '\\[deleted\\]')
# Clean deleted or removed comments
comments <- comments |>
filter(str_match(comment, pattern = '[removed]') != '\\[removed\\]') |>
filter(str_match(comment, pattern = '[deleted]') != '\\[deleted\\]')
# Clean deleted or removed comments
comments <- comments |>
filter(str_match(comment, pattern = '\\[removed\\]') != '[removed]') |>
filter(str_match(comment, pattern = '\\[deleted\\]') != '[deleted]')
# Read threads, then bind them into a nested dataframe using 'purrr::map_dfr'
threads <- fs::dir_ls(path = './data/threads', glob = '*rds') |>
purrr::map_dfr(read_rds)
comments <- threads$comments
threads <- threads$threads
# Clean deleted or removed comments
comments <- comments |>
filter(str_match(comment, pattern = '\\[removed\\]') != '[removed]') |>
filter(str_match(comment, pattern = '\\[deleted\\]') != '[deleted]')
'[removed]'
# Read threads, then bind them into a nested dataframe using 'purrr::map_dfr'
threads <- fs::dir_ls(path = './data/threads', glob = '*rds') |>
purrr::map_dfr(read_rds)
comments <- threads$comments
threads <- threads$threads
# Clean deleted or removed comments
comments <- comments |>
filter(str_extract(comment, pattern = '\\[removed\\]') != '[removed]') |>
filter(str_extract(comment, pattern = '\\[deleted\\]') != '[deleted]')
str_extract(comments$comment[10], pattern = '\\[removed\\]')
str_extract(comments$comment[10], pattern = '[removed]')
str_extract(comments$comment[10], pattern = 'removed')
# Read threads, then bind them into a nested dataframe using 'purrr::map_dfr'
threads <- fs::dir_ls(path = './data/threads', glob = '*rds') |>
purrr::map_dfr(read_rds)
comments <- threads$comments
threads <- threads$threads
str_extract(comments$comment[10], pattern = 'removed')
str_extract(comments$comment[10], pattern = '[removed]')
str_extract(comments$comment[10], pattern = '\\[removed\\]')
str_extract(comments$comment[10], pattern = '\\[deleted\\]')
str_extract(comments$comment[10], pattern = '\\[deleted\\]') == '[deleted]'
# Read threads, then bind them into a nested dataframe using 'purrr::map_dfr'
threads <- fs::dir_ls(path = './data/threads', glob = '*rds') |>
purrr::map_dfr(read_rds)
comments <- threads$comments
threads <- threads$threads
# Clean deleted or removed comments
comments <- comments |>
filter(!str_extract(comment, pattern = '\\[removed\\]') == '[removed]') |>
filter(!str_extract(comment, pattern = '\\[deleted\\]') == '[deleted]')
# Read threads, then bind them into a nested dataframe using 'purrr::map_dfr'
threads <- fs::dir_ls(path = './data/threads', glob = '*rds') |>
purrr::map_dfr(read_rds)
comments <- threads$comments
threads <- threads$threads
# Clean deleted or removed comments
comments <- comments |>
filter(!str_extract(comment, pattern = '[removed]') == '[removed]') |>
filter(!str_extract(comment, pattern = '\\[deleted\\]') == '[deleted]')
comments <- threads$comments
# Read threads, then bind them into a nested dataframe using 'purrr::map_dfr'
threads <- fs::dir_ls(path = './data/threads', glob = '*rds') |>
purrr::map_dfr(read_rds)
comments <- threads$comments
threads <- threads$threads
# Clean deleted or removed comments
comments <- comments |>
filter(!str_detect(comment, pattern = '\\[removed\\]'))
# Clean deleted or removed comments
comments <- comments |>
filter(!str_detect(comment, pattern = '\\[removed\\]')) |>
filter(!str_detect(comment, pattern = '\\[deleted\\]'))
# Read threads, then bind them into a nested dataframe using 'purrr::map_dfr'
threads <- fs::dir_ls(path = './data/threads', glob = '*rds') |>
purrr::map_dfr(read_rds)
comments <- threads$comments
threads <- threads$threads
# Clean deleted or removed comments
comments <- comments |>
filter(!str_detect(comment, pattern = '\\[removed\\]')) |>
filter(!str_detect(comment, pattern = '\\[deleted\\]'))
View(comments)
